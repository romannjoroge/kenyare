[Source 1](https://www.transre.com/ai-and-machine-learning-in-re-insurance-series-setting-the-stage/)
Something I didn't know was that there is a technique known as egularization that can help avoid overfitting of machine learning algorithms. The idea behind regularization is that you decrease the training accuracy of the model in return for an increase in being able to adapt to general data. You decrease accuracy on training data for an increase in accuracy on actual data.

A nice definition of overfitting is when a models accuracy on training data increases but its accuracy on test data does not increase or starts decreasing. [More information on regulatization:](https://www.ibm.com/topics/regularization#:~:text=Regularization%20is%20a%20set%20of,overfitting%20in%20machine%20learning%20models.)

1. Name Matching and Entity Recognition - may not be enough for a single app but sounds important
2. Predicitive analysis (cyber events, catastrophies, impact of class action lawsuits, policy pricing proposals)
3. Portfolio optimization (risk/return optimization, volatility dampening)
4. LLMs for identifying profitable contract language, creating new parameterized insurance products

[Reinsurance News](https://www.reinsurancene.ws/tag/artificial-intelligence/)
1. AI powered risk analytics in underwritting process - computer vision to monitor properties and accurately identify coverage
2. Loss control and premium audit surveys (automate management of risk inspection surveys)
3. Processing of complex data such as Schedules of Value(SOV) and Loss Runs
4. Using GenAI to transform unstractured data from submission-related documents into structured, analysable data
5. Boost claim process by identifying things such as bad faith doctrines, time-sensitive conditions, key medical issues (provide summaries based on salient data within claims, builds treatment timelines, asses risk of claim escalation)
6. Personalization of customer offers

